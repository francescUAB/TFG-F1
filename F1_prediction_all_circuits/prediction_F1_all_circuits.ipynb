{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,mean_absolute_error, r2_score,f1_score, recall_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from functions.functions_f1_all_circuits import process_f1_dataset_single_position, process_f1_dataset_grouped_by_2, process_f1_dataset_grouped_by_4\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\TFG\\TFG GIT\\TFG\\F1_prediction_all_circuits\\functions\\functions_f1_all_circuits.py:62: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sense agrupació guardat en: ./datasets/f1_top20_target_single_position_all_circuits.csv\n",
      "Dataset sense agrupació carregat:\n",
      "   raceid  driverid driverref  constructorid  circuitref  circuitid  grid  \\\n",
      "0     257        57  hakkinen              1  interlagos         18     8   \n",
      "1     257        84   brundle              1  interlagos         18    18   \n",
      "2     258        57  hakkinen              1     okayama         28     4   \n",
      "3     258        84   brundle              1     okayama         28     6   \n",
      "4     259        57  hakkinen              1       imola         21     8   \n",
      "\n",
      "   positionorder        date  year         dob  experience  hability  \\\n",
      "0             21  1994-03-27  1994  1968-09-28          13      80.0   \n",
      "1             14  1994-03-27  1994  1959-06-01          35      74.0   \n",
      "2             21  1994-04-17  1994  1968-09-28          13      72.0   \n",
      "3             15  1994-04-17  1994  1959-06-01          35      74.0   \n",
      "4              3  1994-05-01  1994  1968-09-28          13      72.0   \n",
      "\n",
      "   constructor_experience  constructor_fiability  constructor_performance  \\\n",
      "0                      32                   19.0                     80.0   \n",
      "1                      32                    6.0                     76.0   \n",
      "2                      32                    6.0                     76.0   \n",
      "3                      32                    6.0                     76.0   \n",
      "4                      32                    6.0                     76.0   \n",
      "\n",
      "   gap_to_best_time  age  target  \n",
      "0             2.160   25      21  \n",
      "1             2.902   34      14  \n",
      "2             1.465   25      21  \n",
      "3             2.133   34      15  \n",
      "4             1.592   25       3  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\TFG\\TFG GIT\\TFG\\F1_prediction_all_circuits\\functions\\functions_f1_all_circuits.py:149: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset agrupació 2 guardat en: ./datasets/f1_top20_target_agrupacio_2_all_circuits.csv\n",
      "\n",
      "Dataset agrupat per 2 posicions carregat:\n",
      "   raceid  driverid driverref  constructorid  circuitref  circuitid  grid  \\\n",
      "0     257        57  hakkinen              1  interlagos         18     8   \n",
      "1     257        84   brundle              1  interlagos         18    18   \n",
      "2     258        57  hakkinen              1     okayama         28     4   \n",
      "3     258        84   brundle              1     okayama         28     6   \n",
      "4     259        57  hakkinen              1       imola         21     8   \n",
      "\n",
      "   positionorder        date  year         dob  experience  hability  \\\n",
      "0             21  1994-03-27  1994  1968-09-28          13      80.0   \n",
      "1             14  1994-03-27  1994  1959-06-01          35      74.0   \n",
      "2             21  1994-04-17  1994  1968-09-28          13      72.0   \n",
      "3             15  1994-04-17  1994  1959-06-01          35      74.0   \n",
      "4              3  1994-05-01  1994  1968-09-28          13      72.0   \n",
      "\n",
      "   constructor_experience  constructor_fiability  constructor_performance  \\\n",
      "0                      32                   19.0                     80.0   \n",
      "1                      32                    6.0                     76.0   \n",
      "2                      32                    6.0                     76.0   \n",
      "3                      32                    6.0                     76.0   \n",
      "4                      32                    6.0                     76.0   \n",
      "\n",
      "   gap_to_best_time  age  target  \n",
      "0             2.160   25      11  \n",
      "1             2.902   34       7  \n",
      "2             1.465   25      11  \n",
      "3             2.133   34       8  \n",
      "4             1.592   25       2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\TFG\\TFG GIT\\TFG\\F1_prediction_all_circuits\\functions\\functions_f1_all_circuits.py:149: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset agrupació 4 guardat en: ./datasets/f1_top20_target_agrupacio_4_all_circuits.csv\n",
      "\n",
      "Dataset agrupat per 4 posicions carregat:\n",
      "   raceid  driverid driverref  constructorid  circuitref  circuitid  grid  \\\n",
      "0     257        57  hakkinen              1  interlagos         18     8   \n",
      "1     257        84   brundle              1  interlagos         18    18   \n",
      "2     258        57  hakkinen              1     okayama         28     4   \n",
      "3     258        84   brundle              1     okayama         28     6   \n",
      "4     259        57  hakkinen              1       imola         21     8   \n",
      "\n",
      "   positionorder        date  year         dob  experience  hability  \\\n",
      "0             21  1994-03-27  1994  1968-09-28          13      80.0   \n",
      "1             14  1994-03-27  1994  1959-06-01          35      74.0   \n",
      "2             21  1994-04-17  1994  1968-09-28          13      72.0   \n",
      "3             15  1994-04-17  1994  1959-06-01          35      74.0   \n",
      "4              3  1994-05-01  1994  1968-09-28          13      72.0   \n",
      "\n",
      "   constructor_experience  constructor_fiability  constructor_performance  \\\n",
      "0                      32                   19.0                     80.0   \n",
      "1                      32                    6.0                     76.0   \n",
      "2                      32                    6.0                     76.0   \n",
      "3                      32                    6.0                     76.0   \n",
      "4                      32                    6.0                     76.0   \n",
      "\n",
      "   gap_to_best_time  age  target  \n",
      "0             2.160   25       6  \n",
      "1             2.902   34       4  \n",
      "2             1.465   25       6  \n",
      "3             2.133   34       4  \n",
      "4             1.592   25       1  \n"
     ]
    }
   ],
   "source": [
    "# Cridar la funció per generar el dataset sense agrupació\n",
    "process_f1_dataset_single_position()\n",
    "dataframe_single_position = pd.read_csv('datasets/f1_top20_target_single_position_all_circuits.csv', delimiter=';')\n",
    "print(\"Dataset sense agrupació carregat:\")\n",
    "print(dataframe_single_position.head())\n",
    "\n",
    "# Cridar la funció per generar el dataset agrupat per 2 posicions\n",
    "process_f1_dataset_grouped_by_2()\n",
    "dataframe_grouped_by_2 = pd.read_csv('datasets/f1_top20_target_agrupacio_2_all_circuits.csv', delimiter=';')\n",
    "print(\"\\nDataset agrupat per 2 posicions carregat:\")\n",
    "print(dataframe_grouped_by_2.head())\n",
    "\n",
    "# Cridar la funció per generar el dataset agrupat per 4 posicions\n",
    "process_f1_dataset_grouped_by_4()\n",
    "dataframe_grouped_by_4 = pd.read_csv('datasets/f1_top20_target_agrupacio_4_all_circuits.csv', delimiter=';')\n",
    "print(\"\\nDataset agrupat per 4 posicions carregat:\")\n",
    "print(dataframe_grouped_by_4.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "def objective_nn(trial, df, agrupacio):\n",
    "    df = df[df['year'] >= 2014]\n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitid', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_onehot = encoder.fit_transform(np.array(y_train_smote).reshape(-1, 1))\n",
    "    y_test_onehot = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)\n",
    "    num_neurons = trial.suggest_int('num_neurons', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=X_train_smote.shape[1], activation='relu'))\n",
    "    for _ in range(num_hidden_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(len(encoder.categories_[0]), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train_smote, y_train_onehot, epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    y_test_classes = np.argmax(y_test_onehot, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "datasets = {\n",
    "    1: dataframe_single_position,\n",
    "    2: dataframe_grouped_by_2,\n",
    "    4: dataframe_grouped_by_4,\n",
    "}\n",
    "\n",
    "best_params_per_group = {}\n",
    "for agrupacio, df in datasets.items():\n",
    "    print(f\"\\nOptimitzant xarxa neuronal per agrupació {agrupacio}...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_nn(trial, df, agrupacio), n_trials=50)\n",
    "    best_params_per_group[agrupacio] = study.best_params\n",
    "    print(f\"\\nMillors hiperparàmetres per agrupació {agrupacio}: {study.best_params}\")\n",
    "    print(f\"Millor valor d'accuracy per agrupació {agrupacio}: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant model per agrupació 1...\n",
      "Entrenant amb agrupació 1. Classes després del filtratge: 20\n",
      "Distribució després de SMOTE: target\n",
      "13    173\n",
      "7     173\n",
      "11    173\n",
      "9     173\n",
      "5     173\n",
      "14    173\n",
      "12    173\n",
      "19    173\n",
      "4     173\n",
      "17    173\n",
      "18    173\n",
      "16    173\n",
      "20    173\n",
      "10    173\n",
      "15    173\n",
      "3     173\n",
      "8     173\n",
      "1     173\n",
      "6     173\n",
      "2     173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 1:\n",
      "Accuracy: 0.1449\n",
      "F1-Score: 0.1238\n",
      "Recall: 0.1449\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.77      0.52        43\n",
      "           2       0.21      0.16      0.18        43\n",
      "           3       0.18      0.26      0.21        43\n",
      "           4       0.14      0.21      0.17        43\n",
      "           5       0.11      0.14      0.12        43\n",
      "           6       0.07      0.12      0.09        43\n",
      "           7       0.08      0.12      0.10        43\n",
      "           8       0.20      0.12      0.15        43\n",
      "           9       0.05      0.02      0.03        43\n",
      "          10       0.14      0.12      0.12        43\n",
      "          11       0.00      0.00      0.00        43\n",
      "          12       0.10      0.07      0.08        41\n",
      "          13       0.14      0.14      0.14        42\n",
      "          14       0.11      0.14      0.12        42\n",
      "          15       0.08      0.10      0.09        42\n",
      "          16       0.16      0.31      0.21        42\n",
      "          17       0.07      0.05      0.06        42\n",
      "          18       0.05      0.02      0.03        42\n",
      "          19       0.00      0.00      0.00        42\n",
      "          20       0.12      0.02      0.04        41\n",
      "\n",
      "    accuracy                           0.14       849\n",
      "   macro avg       0.12      0.14      0.12       849\n",
      "weighted avg       0.12      0.14      0.12       849\n",
      "\n",
      "\n",
      "Entrenant model per agrupació 2...\n",
      "Entrenant amb agrupació 2. Classes després del filtratge: 10\n",
      "Distribució després de SMOTE: target\n",
      "4     344\n",
      "9     344\n",
      "7     344\n",
      "5     344\n",
      "1     344\n",
      "8     344\n",
      "6     344\n",
      "10    344\n",
      "3     344\n",
      "2     344\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 2:\n",
      "Accuracy: 0.2650\n",
      "F1-Score: 0.2279\n",
      "Recall: 0.2650\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.77      0.63        86\n",
      "           2       0.42      0.53      0.47        85\n",
      "           3       0.21      0.19      0.20        86\n",
      "           4       0.22      0.35      0.27        86\n",
      "           5       0.15      0.12      0.13        86\n",
      "           6       0.10      0.05      0.06        85\n",
      "           7       0.15      0.21      0.18        84\n",
      "           8       0.23      0.40      0.29        84\n",
      "           9       0.09      0.02      0.04        84\n",
      "          10       0.00      0.00      0.00        83\n",
      "\n",
      "    accuracy                           0.27       849\n",
      "   macro avg       0.21      0.26      0.23       849\n",
      "weighted avg       0.21      0.27      0.23       849\n",
      "\n",
      "\n",
      "Entrenant model per agrupació 4...\n",
      "Entrenant amb agrupació 4. Classes després del filtratge: 5\n",
      "Distribució després de SMOTE: target\n",
      "3    686\n",
      "4    686\n",
      "1    686\n",
      "5    686\n",
      "2    686\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 4:\n",
      "Accuracy: 0.4299\n",
      "F1-Score: 0.3839\n",
      "Recall: 0.4299\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.82      0.73       171\n",
      "           2       0.38      0.47      0.42       171\n",
      "           3       0.26      0.19      0.22       170\n",
      "           4       0.39      0.63      0.48       169\n",
      "           5       0.21      0.04      0.06       168\n",
      "\n",
      "    accuracy                           0.43       849\n",
      "   macro avg       0.38      0.43      0.38       849\n",
      "weighted avg       0.38      0.43      0.38       849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "def entrenar_svm(df, agrupacio):\n",
    "    df = df[df['year'] >= 2014]\n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    print(f\"Entrenant amb agrupació {agrupacio}. Classes després del filtratge: {df['target'].nunique()}\")\n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    print(f\"Distribució després de SMOTE: {y_train_smote.value_counts()}\")\n",
    "    model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"\\nResultats per agrupació {agrupacio}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"\\nInforme de classificació:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "print(\"Entrenant model per agrupació 1...\")\n",
    "model_agrupacio_1 = entrenar_svm(dataframe_single_position, agrupacio=1)\n",
    "print(\"\\nEntrenant model per agrupació 2...\")\n",
    "model_agrupacio_2 = entrenar_svm(dataframe_grouped_by_2, agrupacio=2)\n",
    "print(\"\\nEntrenant model per agrupació 4...\")\n",
    "model_agrupacio_4 = entrenar_svm(dataframe_grouped_by_4, agrupacio=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "def objective_svm(trial, df, agrupacio):\n",
    "    df = df[df['year'] >= 2014]\n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitid', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    C = trial.suggest_loguniform('C', 1e-3, 1e2)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-4, 1e0) if kernel != 'linear' else 'scale'\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, probability=True, random_state=42)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "datasets = {\n",
    "    1: pd.read_csv('datasets/f1_top20_target_single_position_all_circuits.csv', delimiter=';'),\n",
    "    2: pd.read_csv('datasets/f1_top20_target_agrupacio_2_all_circuits.csv', delimiter=';'),\n",
    "    4: pd.read_csv('datasets/f1_top20_target_agrupacio_4_all_circuits.csv', delimiter=';'),\n",
    "}\n",
    "\n",
    "best_params_per_group = {}\n",
    "for agrupacio, df in datasets.items():\n",
    "    print(f\"\\nOptimitzant SVM per agrupació {agrupacio}...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_svm(trial, df, agrupacio), n_trials=50)\n",
    "    best_params_per_group[agrupacio] = study.best_params\n",
    "    print(f\"\\nMillors hiperparàmetres per agrupació {agrupacio}: {study.best_params}\")\n",
    "    print(f\"Millor valor d'accuracy per agrupació {agrupacio}: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 19:48:43,327] A new study created in memory with name: no-name-a45659da-ec55-4654-9bac-aafcf1d068fd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimitzant Random Forest per agrupació 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 19:48:44,324] Trial 0 finished with value: 0.15076560659599528 and parameters: {'n_estimators': 151, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.15076560659599528.\n",
      "[I 2025-02-02 19:48:44,686] Trial 1 finished with value: 0.15783274440518258 and parameters: {'n_estimators': 110, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:45,009] Trial 2 finished with value: 0.15665488810365136 and parameters: {'n_estimators': 109, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:45,676] Trial 3 finished with value: 0.1519434628975265 and parameters: {'n_estimators': 252, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:46,801] Trial 4 finished with value: 0.14958775029446408 and parameters: {'n_estimators': 293, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:47,015] Trial 5 finished with value: 0.15076560659599528 and parameters: {'n_estimators': 69, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:47,610] Trial 6 finished with value: 0.15076560659599528 and parameters: {'n_estimators': 104, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:47,824] Trial 7 finished with value: 0.1425206124852768 and parameters: {'n_estimators': 77, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:48,087] Trial 8 finished with value: 0.15783274440518258 and parameters: {'n_estimators': 54, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:48,857] Trial 9 finished with value: 0.15547703180212014 and parameters: {'n_estimators': 217, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.15783274440518258.\n",
      "[I 2025-02-02 19:48:49,637] Trial 10 finished with value: 0.16136631330977622 and parameters: {'n_estimators': 164, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 8}. Best is trial 10 with value: 0.16136631330977622.\n",
      "[I 2025-02-02 19:48:50,393] Trial 11 finished with value: 0.15901060070671377 and parameters: {'n_estimators': 164, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 10 with value: 0.16136631330977622.\n",
      "[I 2025-02-02 19:48:51,316] Trial 12 finished with value: 0.16489988221436985 and parameters: {'n_estimators': 176, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 12 with value: 0.16489988221436985.\n",
      "[I 2025-02-02 19:48:52,317] Trial 13 finished with value: 0.16136631330977622 and parameters: {'n_estimators': 207, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 12 with value: 0.16489988221436985.\n",
      "[I 2025-02-02 19:48:53,167] Trial 14 finished with value: 0.160188457008245 and parameters: {'n_estimators': 189, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 12 with value: 0.16489988221436985.\n",
      "[I 2025-02-02 19:48:53,976] Trial 15 finished with value: 0.15429917550058891 and parameters: {'n_estimators': 161, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 12 with value: 0.16489988221436985.\n",
      "[I 2025-02-02 19:48:54,573] Trial 16 finished with value: 0.1684334511189635 and parameters: {'n_estimators': 136, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:55,109] Trial 17 finished with value: 0.1625441696113074 and parameters: {'n_estimators': 129, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:56,283] Trial 18 finished with value: 0.15312131919905772 and parameters: {'n_estimators': 232, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:56,852] Trial 19 finished with value: 0.16489988221436985 and parameters: {'n_estimators': 128, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:57,859] Trial 20 finished with value: 0.16372202591283863 and parameters: {'n_estimators': 190, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:58,478] Trial 21 finished with value: 0.16489988221436985 and parameters: {'n_estimators': 137, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:59,014] Trial 22 finished with value: 0.15547703180212014 and parameters: {'n_estimators': 130, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:48:59,491] Trial 23 finished with value: 0.16372202591283863 and parameters: {'n_estimators': 91, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:00,206] Trial 24 finished with value: 0.15429917550058891 and parameters: {'n_estimators': 142, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:00,979] Trial 25 finished with value: 0.16607773851590105 and parameters: {'n_estimators': 189, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:01,906] Trial 26 finished with value: 0.1684334511189635 and parameters: {'n_estimators': 183, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:03,113] Trial 27 finished with value: 0.15547703180212014 and parameters: {'n_estimators': 195, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:03,723] Trial 28 finished with value: 0.15665488810365136 and parameters: {'n_estimators': 252, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:05,161] Trial 29 finished with value: 0.15665488810365136 and parameters: {'n_estimators': 230, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:06,468] Trial 30 finished with value: 0.15547703180212014 and parameters: {'n_estimators': 180, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:07,219] Trial 31 finished with value: 0.16489988221436985 and parameters: {'n_estimators': 155, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:07,962] Trial 32 finished with value: 0.15901060070671377 and parameters: {'n_estimators': 177, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:09,031] Trial 33 finished with value: 0.15901060070671377 and parameters: {'n_estimators': 203, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:09,789] Trial 34 finished with value: 0.1625441696113074 and parameters: {'n_estimators': 177, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:10,475] Trial 35 finished with value: 0.1684334511189635 and parameters: {'n_estimators': 148, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:10,877] Trial 36 finished with value: 0.15783274440518258 and parameters: {'n_estimators': 115, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:11,510] Trial 37 finished with value: 0.15665488810365136 and parameters: {'n_estimators': 147, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:13,124] Trial 38 finished with value: 0.15901060070671377 and parameters: {'n_estimators': 292, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:13,634] Trial 39 finished with value: 0.1519434628975265 and parameters: {'n_estimators': 101, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:14,008] Trial 40 finished with value: 0.15547703180212014 and parameters: {'n_estimators': 117, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:14,806] Trial 41 finished with value: 0.15901060070671377 and parameters: {'n_estimators': 151, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:15,891] Trial 42 finished with value: 0.1684334511189635 and parameters: {'n_estimators': 220, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:16,763] Trial 43 finished with value: 0.14723203769140164 and parameters: {'n_estimators': 222, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:17,985] Trial 44 finished with value: 0.15901060070671377 and parameters: {'n_estimators': 256, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:19,133] Trial 45 finished with value: 0.15547703180212014 and parameters: {'n_estimators': 210, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:20,267] Trial 46 finished with value: 0.1625441696113074 and parameters: {'n_estimators': 236, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:21,081] Trial 47 finished with value: 0.16136631330977622 and parameters: {'n_estimators': 168, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:22,191] Trial 48 finished with value: 0.15783274440518258 and parameters: {'n_estimators': 195, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:23,579] Trial 49 finished with value: 0.16136631330977622 and parameters: {'n_estimators': 262, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 16 with value: 0.1684334511189635.\n",
      "[I 2025-02-02 19:49:23,580] A new study created in memory with name: no-name-c31fbe23-8c8f-428f-a523-86a5bb553904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Millors hiperparàmetres per agrupació 1: {'n_estimators': 136, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 6}\n",
      "Millor valor d'accuracy per agrupació 1: 0.1684\n",
      "\n",
      "Optimitzant Random Forest per agrupació 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 19:49:25,180] Trial 0 finished with value: 0.2767962308598351 and parameters: {'n_estimators': 272, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.2767962308598351.\n",
      "[I 2025-02-02 19:49:25,530] Trial 1 finished with value: 0.29328621908127206 and parameters: {'n_estimators': 108, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.29328621908127206.\n",
      "[I 2025-02-02 19:49:25,891] Trial 2 finished with value: 0.29799764428739695 and parameters: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:26,826] Trial 3 finished with value: 0.28032979976442873 and parameters: {'n_estimators': 204, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:27,729] Trial 4 finished with value: 0.28975265017667845 and parameters: {'n_estimators': 241, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:28,428] Trial 5 finished with value: 0.29328621908127206 and parameters: {'n_estimators': 267, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:29,008] Trial 6 finished with value: 0.2850412249705536 and parameters: {'n_estimators': 205, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:29,541] Trial 7 finished with value: 0.26855123674911663 and parameters: {'n_estimators': 92, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:30,914] Trial 8 finished with value: 0.2838633686690224 and parameters: {'n_estimators': 290, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:31,164] Trial 9 finished with value: 0.2944640753828033 and parameters: {'n_estimators': 70, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:31,937] Trial 10 finished with value: 0.2862190812720848 and parameters: {'n_estimators': 138, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.29799764428739695.\n",
      "[I 2025-02-02 19:49:32,164] Trial 11 finished with value: 0.30742049469964666 and parameters: {'n_estimators': 50, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:32,392] Trial 12 finished with value: 0.3003533568904594 and parameters: {'n_estimators': 52, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:32,634] Trial 13 finished with value: 0.29328621908127206 and parameters: {'n_estimators': 52, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:33,359] Trial 14 finished with value: 0.2862190812720848 and parameters: {'n_estimators': 158, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:33,644] Trial 15 finished with value: 0.29328621908127206 and parameters: {'n_estimators': 51, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:34,225] Trial 16 finished with value: 0.29093050647820967 and parameters: {'n_estimators': 125, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:34,498] Trial 17 finished with value: 0.2956419316843345 and parameters: {'n_estimators': 80, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:35,429] Trial 18 finished with value: 0.28975265017667845 and parameters: {'n_estimators': 182, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:35,797] Trial 19 finished with value: 0.2921083627797409 and parameters: {'n_estimators': 133, 'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:36,143] Trial 20 finished with value: 0.2756183745583039 and parameters: {'n_estimators': 67, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:36,555] Trial 21 finished with value: 0.29799764428739695 and parameters: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:36,949] Trial 22 finished with value: 0.2862190812720848 and parameters: {'n_estimators': 80, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:37,403] Trial 23 finished with value: 0.287396937573616 and parameters: {'n_estimators': 112, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:37,596] Trial 24 finished with value: 0.29093050647820967 and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:38,000] Trial 25 finished with value: 0.2956419316843345 and parameters: {'n_estimators': 80, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:38,392] Trial 26 finished with value: 0.2838633686690224 and parameters: {'n_estimators': 96, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:38,864] Trial 27 finished with value: 0.28857479387514723 and parameters: {'n_estimators': 157, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:39,188] Trial 28 finished with value: 0.2956419316843345 and parameters: {'n_estimators': 64, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:39,874] Trial 29 finished with value: 0.2661955241460542 and parameters: {'n_estimators': 120, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:40,449] Trial 30 finished with value: 0.29093050647820967 and parameters: {'n_estimators': 148, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:40,769] Trial 31 finished with value: 0.2944640753828033 and parameters: {'n_estimators': 93, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:41,027] Trial 32 finished with value: 0.2921083627797409 and parameters: {'n_estimators': 102, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:41,298] Trial 33 finished with value: 0.2968197879858657 and parameters: {'n_estimators': 67, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:42,081] Trial 34 finished with value: 0.2826855123674912 and parameters: {'n_estimators': 177, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:42,472] Trial 35 finished with value: 0.28857479387514723 and parameters: {'n_estimators': 111, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:43,390] Trial 36 finished with value: 0.2767962308598351 and parameters: {'n_estimators': 216, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:43,629] Trial 37 finished with value: 0.2968197879858657 and parameters: {'n_estimators': 84, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:43,885] Trial 38 finished with value: 0.2697290930506478 and parameters: {'n_estimators': 60, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:44,388] Trial 39 finished with value: 0.28975265017667845 and parameters: {'n_estimators': 105, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:45,346] Trial 40 finished with value: 0.2956419316843345 and parameters: {'n_estimators': 263, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:45,657] Trial 41 finished with value: 0.29328621908127206 and parameters: {'n_estimators': 71, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:45,901] Trial 42 finished with value: 0.28975265017667845 and parameters: {'n_estimators': 68, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:46,273] Trial 43 finished with value: 0.2850412249705536 and parameters: {'n_estimators': 89, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:46,476] Trial 44 finished with value: 0.2921083627797409 and parameters: {'n_estimators': 59, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:46,824] Trial 45 finished with value: 0.29093050647820967 and parameters: {'n_estimators': 77, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:47,130] Trial 46 finished with value: 0.2791519434628975 and parameters: {'n_estimators': 58, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:47,474] Trial 47 finished with value: 0.2944640753828033 and parameters: {'n_estimators': 123, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:47,843] Trial 48 finished with value: 0.2944640753828033 and parameters: {'n_estimators': 98, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:48,812] Trial 49 finished with value: 0.28857479387514723 and parameters: {'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.30742049469964666.\n",
      "[I 2025-02-02 19:49:48,814] A new study created in memory with name: no-name-bc98722a-0ef5-468a-ad95-acde51bc0799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Millors hiperparàmetres per agrupació 2: {'n_estimators': 50, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 7}\n",
      "Millor valor d'accuracy per agrupació 2: 0.3074\n",
      "\n",
      "Optimitzant Random Forest per agrupació 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 19:49:49,090] Trial 0 finished with value: 0.4793875147232038 and parameters: {'n_estimators': 61, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:49,466] Trial 1 finished with value: 0.4734982332155477 and parameters: {'n_estimators': 87, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:50,029] Trial 2 finished with value: 0.45936395759717313 and parameters: {'n_estimators': 123, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:50,513] Trial 3 finished with value: 0.46878680800942285 and parameters: {'n_estimators': 109, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:51,022] Trial 4 finished with value: 0.4581861012956419 and parameters: {'n_estimators': 150, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:51,897] Trial 5 finished with value: 0.45936395759717313 and parameters: {'n_estimators': 288, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:53,046] Trial 6 finished with value: 0.464075382803298 and parameters: {'n_estimators': 248, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:53,545] Trial 7 finished with value: 0.4534746760895171 and parameters: {'n_estimators': 93, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:54,513] Trial 8 finished with value: 0.46996466431095407 and parameters: {'n_estimators': 168, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:55,058] Trial 9 finished with value: 0.46878680800942285 and parameters: {'n_estimators': 121, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:55,204] Trial 10 finished with value: 0.4464075382803298 and parameters: {'n_estimators': 53, 'max_depth': 2, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.4793875147232038.\n",
      "[I 2025-02-02 19:49:55,445] Trial 11 finished with value: 0.4829210836277974 and parameters: {'n_estimators': 51, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:55,706] Trial 12 finished with value: 0.4817432273262662 and parameters: {'n_estimators': 54, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:56,020] Trial 13 finished with value: 0.4711425206124853 and parameters: {'n_estimators': 61, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:56,883] Trial 14 finished with value: 0.4617196702002356 and parameters: {'n_estimators': 224, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:57,756] Trial 15 finished with value: 0.4628975265017668 and parameters: {'n_estimators': 199, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:58,446] Trial 16 finished with value: 0.4664310954063604 and parameters: {'n_estimators': 148, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:58,830] Trial 17 finished with value: 0.46760895170789163 and parameters: {'n_estimators': 81, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:59,085] Trial 18 finished with value: 0.45936395759717313 and parameters: {'n_estimators': 52, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:49:59,913] Trial 19 finished with value: 0.46996466431095407 and parameters: {'n_estimators': 193, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:00,460] Trial 20 finished with value: 0.4723203769140165 and parameters: {'n_estimators': 139, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:00,817] Trial 21 finished with value: 0.4829210836277974 and parameters: {'n_estimators': 77, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:01,190] Trial 22 finished with value: 0.48056537102473496 and parameters: {'n_estimators': 81, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:01,673] Trial 23 finished with value: 0.4758539458186101 and parameters: {'n_estimators': 105, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:02,021] Trial 24 finished with value: 0.4534746760895171 and parameters: {'n_estimators': 71, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:02,462] Trial 25 finished with value: 0.46996466431095407 and parameters: {'n_estimators': 99, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:02,709] Trial 26 finished with value: 0.45700824499411075 and parameters: {'n_estimators': 50, 'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:03,034] Trial 27 finished with value: 0.46878680800942285 and parameters: {'n_estimators': 74, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:03,271] Trial 28 finished with value: 0.4464075382803298 and parameters: {'n_estimators': 72, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:03,802] Trial 29 finished with value: 0.46760895170789163 and parameters: {'n_estimators': 117, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:04,091] Trial 30 finished with value: 0.4652532391048292 and parameters: {'n_estimators': 64, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.4829210836277974.\n",
      "[I 2025-02-02 19:50:04,494] Trial 31 finished with value: 0.48527679623085984 and parameters: {'n_estimators': 87, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:04,920] Trial 32 finished with value: 0.46760895170789163 and parameters: {'n_estimators': 92, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:05,518] Trial 33 finished with value: 0.4723203769140165 and parameters: {'n_estimators': 135, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:05,922] Trial 34 finished with value: 0.4758539458186101 and parameters: {'n_estimators': 89, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:06,250] Trial 35 finished with value: 0.4628975265017668 and parameters: {'n_estimators': 66, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:06,715] Trial 36 finished with value: 0.4628975265017668 and parameters: {'n_estimators': 101, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:07,983] Trial 37 finished with value: 0.45700824499411075 and parameters: {'n_estimators': 280, 'max_depth': 17, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:08,364] Trial 38 finished with value: 0.47820965842167257 and parameters: {'n_estimators': 83, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:08,855] Trial 39 finished with value: 0.4746760895170789 and parameters: {'n_estimators': 112, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:09,739] Trial 40 finished with value: 0.4817432273262662 and parameters: {'n_estimators': 171, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:10,534] Trial 41 finished with value: 0.4793875147232038 and parameters: {'n_estimators': 181, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:11,213] Trial 42 finished with value: 0.46878680800942285 and parameters: {'n_estimators': 165, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:12,316] Trial 43 finished with value: 0.4652532391048292 and parameters: {'n_estimators': 244, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:12,630] Trial 44 finished with value: 0.45936395759717313 and parameters: {'n_estimators': 60, 'max_depth': 14, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:13,215] Trial 45 finished with value: 0.4746760895170789 and parameters: {'n_estimators': 130, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:13,508] Trial 46 finished with value: 0.4546525323910483 and parameters: {'n_estimators': 60, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:14,579] Trial 47 finished with value: 0.4711425206124853 and parameters: {'n_estimators': 224, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:15,888] Trial 48 finished with value: 0.46996466431095407 and parameters: {'n_estimators': 298, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 31 with value: 0.48527679623085984.\n",
      "[I 2025-02-02 19:50:16,581] Trial 49 finished with value: 0.4664310954063604 and parameters: {'n_estimators': 157, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 31 with value: 0.48527679623085984.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Millors hiperparàmetres per agrupació 4: {'n_estimators': 87, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 8}\n",
      "Millor valor d'accuracy per agrupació 4: 0.4853\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def objective_rf(trial, df, agrupacio):\n",
    "    df = df[df['year'] >= 2014]\n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitid', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "datasets = {\n",
    "    1: pd.read_csv('datasets/f1_top20_target_single_position_all_circuits.csv', delimiter=';'),\n",
    "    2: pd.read_csv('datasets/f1_top20_target_agrupacio_2_all_circuits.csv', delimiter=';'),\n",
    "    4: pd.read_csv('datasets/f1_top20_target_agrupacio_4_all_circuits.csv', delimiter=';'),\n",
    "}\n",
    "\n",
    "best_params_per_group = {}\n",
    "for agrupacio, df in datasets.items():\n",
    "    print(f\"\\nOptimitzant Random Forest per agrupació {agrupacio}...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_rf(trial, df, agrupacio), n_trials=50)\n",
    "    best_params_per_group[agrupacio] = study.best_params\n",
    "    print(f\"\\nMillors hiperparàmetres per agrupació {agrupacio}: {study.best_params}\")\n",
    "    print(f\"Millor valor d'accuracy per agrupació {agrupacio}: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant model per agrupació 1...\n",
      "Entrenant amb agrupació 1. Classes després del filtratge: 20\n",
      "Distribució després de SMOTE: target\n",
      "13    173\n",
      "7     173\n",
      "11    173\n",
      "9     173\n",
      "5     173\n",
      "14    173\n",
      "12    173\n",
      "19    173\n",
      "4     173\n",
      "17    173\n",
      "18    173\n",
      "16    173\n",
      "20    173\n",
      "10    173\n",
      "15    173\n",
      "3     173\n",
      "8     173\n",
      "1     173\n",
      "6     173\n",
      "2     173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 1:\n",
      "Accuracy: 0.1555\n",
      "F1-Score: 0.1491\n",
      "Recall: 0.1555\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.58      0.54        43\n",
      "           2       0.28      0.23      0.25        43\n",
      "           3       0.17      0.21      0.19        43\n",
      "           4       0.16      0.16      0.16        43\n",
      "           5       0.10      0.09      0.09        43\n",
      "           6       0.20      0.28      0.24        43\n",
      "           7       0.20      0.28      0.23        43\n",
      "           8       0.19      0.19      0.19        43\n",
      "           9       0.08      0.07      0.07        43\n",
      "          10       0.17      0.16      0.17        43\n",
      "          11       0.11      0.07      0.09        43\n",
      "          12       0.03      0.02      0.03        41\n",
      "          13       0.14      0.17      0.15        42\n",
      "          14       0.12      0.12      0.12        42\n",
      "          15       0.06      0.07      0.06        42\n",
      "          16       0.11      0.12      0.12        42\n",
      "          17       0.06      0.07      0.07        42\n",
      "          18       0.14      0.14      0.14        42\n",
      "          19       0.06      0.05      0.05        42\n",
      "          20       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.16       849\n",
      "   macro avg       0.14      0.15      0.15       849\n",
      "weighted avg       0.15      0.16      0.15       849\n",
      "\n",
      "\n",
      "Entrenant model per agrupació 2...\n",
      "Entrenant amb agrupació 2. Classes després del filtratge: 10\n",
      "Distribució després de SMOTE: target\n",
      "4     344\n",
      "9     344\n",
      "7     344\n",
      "5     344\n",
      "1     344\n",
      "8     344\n",
      "6     344\n",
      "10    344\n",
      "3     344\n",
      "2     344\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 2:\n",
      "Accuracy: 0.2850\n",
      "F1-Score: 0.2785\n",
      "Recall: 0.2850\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.76      0.69        86\n",
      "           2       0.40      0.45      0.42        85\n",
      "           3       0.31      0.30      0.31        86\n",
      "           4       0.26      0.27      0.26        86\n",
      "           5       0.14      0.16      0.15        86\n",
      "           6       0.12      0.09      0.10        85\n",
      "           7       0.22      0.20      0.21        84\n",
      "           8       0.23      0.26      0.24        84\n",
      "           9       0.26      0.20      0.23        84\n",
      "          10       0.18      0.14      0.16        83\n",
      "\n",
      "    accuracy                           0.29       849\n",
      "   macro avg       0.27      0.28      0.28       849\n",
      "weighted avg       0.28      0.29      0.28       849\n",
      "\n",
      "\n",
      "Entrenant model per agrupació 4...\n",
      "Entrenant amb agrupació 4. Classes després del filtratge: 5\n",
      "Distribució després de SMOTE: target\n",
      "3    686\n",
      "4    686\n",
      "1    686\n",
      "5    686\n",
      "2    686\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 4:\n",
      "Accuracy: 0.4511\n",
      "F1-Score: 0.4437\n",
      "Recall: 0.4511\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.80      0.73       171\n",
      "           2       0.41      0.42      0.41       171\n",
      "           3       0.36      0.36      0.36       170\n",
      "           4       0.40      0.38      0.39       169\n",
      "           5       0.36      0.29      0.32       168\n",
      "\n",
      "    accuracy                           0.45       849\n",
      "   macro avg       0.44      0.45      0.44       849\n",
      "weighted avg       0.44      0.45      0.44       849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "def entrenar_random_forest(df, agrupacio):\n",
    "    df = df[df['year'] >= 2014]\n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    print(f\"Entrenant amb agrupació {agrupacio}. Classes després del filtratge: {df['target'].nunique()}\")\n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    print(f\"Distribució després de SMOTE: {y_train_smote.value_counts()}\")\n",
    "    model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"\\nResultats per agrupació {agrupacio}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"\\nInforme de classificació:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "print(\"Entrenant model per agrupació 1...\")\n",
    "model_rf_agrupacio_1 = entrenar_random_forest(dataframe_single_position, agrupacio=1)\n",
    "print(\"\\nEntrenant model per agrupació 2...\")\n",
    "model_rf_agrupacio_2 = entrenar_random_forest(dataframe_grouped_by_2, agrupacio=2)\n",
    "print(\"\\nEntrenant model per agrupació 4...\")\n",
    "model_rf_agrupacio_4 = entrenar_random_forest(dataframe_grouped_by_4, agrupacio=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant model per agrupació 1...\n",
      "Entrenant amb agrupació 1. Classes després del filtratge: 20\n",
      "Distribució després de SMOTE: target\n",
      "13    173\n",
      "7     173\n",
      "11    173\n",
      "9     173\n",
      "5     173\n",
      "14    173\n",
      "12    173\n",
      "19    173\n",
      "4     173\n",
      "17    173\n",
      "18    173\n",
      "16    173\n",
      "20    173\n",
      "10    173\n",
      "15    173\n",
      "3     173\n",
      "8     173\n",
      "1     173\n",
      "6     173\n",
      "2     173\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Resultats per agrupació 1:\n",
      "Accuracy: 0.1484\n",
      "F1-Score: 0.1359\n",
      "Recall: 0.1484\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.70      0.55        43\n",
      "           1       0.28      0.28      0.28        43\n",
      "           2       0.12      0.16      0.14        43\n",
      "           3       0.15      0.19      0.17        43\n",
      "           4       0.08      0.07      0.07        43\n",
      "           5       0.12      0.19      0.15        43\n",
      "           6       0.12      0.16      0.14        43\n",
      "           7       0.18      0.14      0.16        43\n",
      "           8       0.08      0.09      0.08        43\n",
      "           9       0.08      0.07      0.07        43\n",
      "          10       0.13      0.09      0.11        43\n",
      "          11       0.07      0.07      0.07        41\n",
      "          12       0.20      0.14      0.17        42\n",
      "          13       0.09      0.07      0.08        42\n",
      "          14       0.10      0.24      0.14        42\n",
      "          15       0.14      0.12      0.13        42\n",
      "          16       0.09      0.07      0.08        42\n",
      "          17       0.17      0.10      0.12        42\n",
      "          18       0.00      0.00      0.00        42\n",
      "          19       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.15       849\n",
      "   macro avg       0.13      0.15      0.14       849\n",
      "weighted avg       0.13      0.15      0.14       849\n",
      "\n",
      "\n",
      "Entrenant model per agrupació 2...\n",
      "Entrenant amb agrupació 2. Classes després del filtratge: 10\n",
      "Distribució després de SMOTE: target\n",
      "4     344\n",
      "9     344\n",
      "7     344\n",
      "5     344\n",
      "1     344\n",
      "8     344\n",
      "6     344\n",
      "10    344\n",
      "3     344\n",
      "2     344\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Resultats per agrupació 2:\n",
      "Accuracy: 0.2898\n",
      "F1-Score: 0.2668\n",
      "Recall: 0.2898\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.83      0.69        86\n",
      "           1       0.41      0.47      0.44        85\n",
      "           2       0.28      0.27      0.28        86\n",
      "           3       0.25      0.34      0.29        86\n",
      "           4       0.20      0.20      0.20        86\n",
      "           5       0.13      0.14      0.14        85\n",
      "           6       0.21      0.15      0.18        84\n",
      "           7       0.19      0.29      0.23        84\n",
      "           8       0.24      0.20      0.22        84\n",
      "           9       0.00      0.00      0.00        83\n",
      "\n",
      "    accuracy                           0.29       849\n",
      "   macro avg       0.25      0.29      0.27       849\n",
      "weighted avg       0.25      0.29      0.27       849\n",
      "\n",
      "\n",
      "Entrenant model per agrupació 4...\n",
      "Entrenant amb agrupació 4. Classes després del filtratge: 5\n",
      "Distribució després de SMOTE: target\n",
      "3    686\n",
      "4    686\n",
      "1    686\n",
      "5    686\n",
      "2    686\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Resultats per agrupació 4:\n",
      "Accuracy: 0.4853\n",
      "F1-Score: 0.4691\n",
      "Recall: 0.4853\n",
      "\n",
      "Informe de classificació:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.84      0.76       171\n",
      "           1       0.46      0.53      0.49       171\n",
      "           2       0.36      0.30      0.33       170\n",
      "           3       0.40      0.51      0.45       169\n",
      "           4       0.49      0.24      0.32       168\n",
      "\n",
      "    accuracy                           0.49       849\n",
      "   macro avg       0.48      0.48      0.47       849\n",
      "weighted avg       0.48      0.49      0.47       849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def entrenar_neural_network(df, agrupacio):\n",
    "    \n",
    "    df = df[df['year'] >= 2014]\n",
    "    \n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    \n",
    "    print(f\"Entrenant amb agrupació {agrupacio}. Classes després del filtratge: {df['target'].nunique()}\")\n",
    "    \n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitid', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"Distribució després de SMOTE: {y_train_smote.value_counts()}\")\n",
    "    \n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_onehot = encoder.fit_transform(np.array(y_train_smote).reshape(-1, 1))\n",
    "    y_test_onehot = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=X_train_smote.shape[1], activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(len(encoder.categories_[0]), activation='softmax')  # Nombre de sortides igual al nombre de classes\n",
    "    ])\n",
    "    \n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train_smote, y_train_onehot,\n",
    "                        validation_data=(X_test_scaled, y_test_onehot),\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        verbose=0)\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    y_test_classes = np.argmax(y_test_onehot, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "    \n",
    "    print(f\"\\nResultats per agrupació {agrupacio}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"\\nInforme de classificació:\")\n",
    "    print(classification_report(y_test_classes, y_pred_classes))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Entrenant model per agrupació 1...\")\n",
    "model_nn_agrupacio_1, history_nn_agrupacio_1 = entrenar_neural_network(dataframe_single_position, agrupacio=1)\n",
    "\n",
    "print(\"\\nEntrenant model per agrupació 2...\")\n",
    "model_nn_agrupacio_2, history_nn_agrupacio_2 = entrenar_neural_network(dataframe_grouped_by_2, agrupacio=2)\n",
    "\n",
    "print(\"\\nEntrenant model per agrupació 4...\")\n",
    "model_nn_agrupacio_4, history_nn_agrupacio_4 = entrenar_neural_network(dataframe_grouped_by_4, agrupacio=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "def objective_nn(trial, df, agrupacio):\n",
    "\n",
    "    df = df[df['year'] >= 2014]\n",
    "    \n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    \n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitid', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_onehot = encoder.fit_transform(np.array(y_train_smote).reshape(-1, 1))\n",
    "    y_test_onehot = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)\n",
    "    num_neurons = trial.suggest_int('num_neurons', 32, 128)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=X_train_smote.shape[1], activation='relu'))\n",
    "    for _ in range(num_hidden_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(len(encoder.categories_[0]), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train_smote, y_train_onehot, epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test_scaled)\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    y_test_classes = np.argmax(y_test_onehot, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    1: dataframe_single_position, \n",
    "    2: dataframe_grouped_by_2,    \n",
    "    4: dataframe_grouped_by_4,   \n",
    "}\n",
    "\n",
    "\n",
    "best_params_per_group = {}\n",
    "for agrupacio, df in datasets.items():\n",
    "    print(f\"\\nOptimitzant xarxa neuronal per agrupació {agrupacio}...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective_nn(trial, df, agrupacio), n_trials=50)\n",
    "    \n",
    "    \n",
    "    best_params_per_group[agrupacio] = study.best_params\n",
    "    print(f\"\\nMillors hiperparàmetres per agrupació {agrupacio}: {study.best_params}\")\n",
    "    print(f\"Millor valor d'accuracy per agrupació {agrupacio}: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant model per agrupació 1...\n",
      "Entrenant amb agrupació 1. Clases después del filtrado: 20\n",
      "Columnas usadas en el entrenamiento SVM: ['constructorid', 'circuitid', 'grid', 'experience', 'hability', 'constructor_experience', 'constructor_fiability', 'constructor_performance', 'gap_to_best_time', 'age']\n",
      "Distribución después de SMOTE:\n",
      "target\n",
      "13    173\n",
      "7     173\n",
      "11    173\n",
      "9     173\n",
      "5     173\n",
      "14    173\n",
      "12    173\n",
      "19    173\n",
      "4     173\n",
      "17    173\n",
      "18    173\n",
      "16    173\n",
      "20    173\n",
      "10    173\n",
      "15    173\n",
      "3     173\n",
      "8     173\n",
      "1     173\n",
      "6     173\n",
      "2     173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultats per agrupació 1:\n",
      "Accuracy: 0.1449\n",
      "F1-Score: 0.1238\n",
      "Recall: 0.1449\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.77      0.52        43\n",
      "           2       0.21      0.16      0.18        43\n",
      "           3       0.18      0.26      0.21        43\n",
      "           4       0.14      0.21      0.17        43\n",
      "           5       0.11      0.14      0.12        43\n",
      "           6       0.07      0.12      0.09        43\n",
      "           7       0.08      0.12      0.10        43\n",
      "           8       0.20      0.12      0.15        43\n",
      "           9       0.05      0.02      0.03        43\n",
      "          10       0.14      0.12      0.12        43\n",
      "          11       0.00      0.00      0.00        43\n",
      "          12       0.10      0.07      0.08        41\n",
      "          13       0.14      0.14      0.14        42\n",
      "          14       0.11      0.14      0.12        42\n",
      "          15       0.08      0.10      0.09        42\n",
      "          16       0.16      0.31      0.21        42\n",
      "          17       0.07      0.05      0.06        42\n",
      "          18       0.05      0.02      0.03        42\n",
      "          19       0.00      0.00      0.00        42\n",
      "          20       0.12      0.02      0.04        41\n",
      "\n",
      "    accuracy                           0.14       849\n",
      "   macro avg       0.12      0.14      0.12       849\n",
      "weighted avg       0.12      0.14      0.12       849\n",
      "\n",
      "\n",
      "¡Modelos SVM entrenats i guardats amb èxit!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def entrenar_svm(df, agrupacio):\n",
    "    \"\"\"\n",
    "    Entrena un modelo SVM usando las siguientes características:\n",
    "      - constructorid\n",
    "      - circuitid\n",
    "      - grid\n",
    "      - experience\n",
    "      - hability\n",
    "      - constructor_experience\n",
    "      - constructor_fiability\n",
    "      - constructor_performance\n",
    "      - gap_to_best_time\n",
    "      - age\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[df['year'] >= 2014]\n",
    "    \n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    print(f\"Entrenant amb agrupació {agrupacio}. Clases después del filtrado: {df['target'].nunique()}\")\n",
    "    \n",
    "    features = [\n",
    "        'constructorid',\n",
    "        'circuitid',\n",
    "        'grid',\n",
    "        'experience',\n",
    "        'hability',\n",
    "        'constructor_experience',\n",
    "        'constructor_fiability',\n",
    "        'constructor_performance',\n",
    "        'gap_to_best_time',\n",
    "        'age'\n",
    "    ]\n",
    "    \n",
    "    missing = [col for col in features if col not in df.columns]\n",
    "    if missing:\n",
    "        print(\"Faltan las siguientes columnas en el DataFrame:\", missing)\n",
    "        return None, None\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "    print(\"Columnas usadas en el entrenamiento SVM:\", list(X.columns))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    print(\"Distribución después de SMOTE:\")\n",
    "    print(pd.Series(y_train_smote).value_counts())\n",
    "    \n",
    "    model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\nResultats per agrupació {agrupacio}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "print(\"Entrenant model per agrupació 1...\")\n",
    "model_agrupacio_1, scaler_1 = entrenar_svm(dataframe_single_position, agrupacio=1)\n",
    "\n",
    "ruta_modelo_1 = r'C:\\Users\\Lenovo\\Desktop\\TFG\\TFG GIT\\TFG\\F1_prediction_all_circuits\\SVM models\\svm_model_agrupacio_1.pkl'\n",
    "ruta_scaler_1 = r'C:\\Users\\Lenovo\\Desktop\\TFG\\TFG GIT\\TFG\\F1_prediction_all_circuits\\SVM models\\scaler_agrupacio_1.pkl'\n",
    "\n",
    "joblib.dump(model_agrupacio_1, os.path.abspath(ruta_modelo_1))\n",
    "joblib.dump(scaler_1, os.path.abspath(ruta_scaler_1))\n",
    "\n",
    "print(\"\\n¡Modelos SVM entrenats i guardats amb èxit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo Random Forest para agrupación 1...\n",
      "Entrenando modelo para agrupación 1. Clases tras el filtrado: 20\n",
      "Distribución tras SMOTE:\n",
      "target\n",
      "13    173\n",
      "7     173\n",
      "11    173\n",
      "9     173\n",
      "5     173\n",
      "14    173\n",
      "12    173\n",
      "19    173\n",
      "4     173\n",
      "17    173\n",
      "18    173\n",
      "16    173\n",
      "20    173\n",
      "10    173\n",
      "15    173\n",
      "3     173\n",
      "8     173\n",
      "1     173\n",
      "6     173\n",
      "2     173\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultados para agrupación 1:\n",
      "Accuracy: 0.1555\n",
      "F1-Score: 0.1491\n",
      "Recall: 0.1555\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.58      0.54        43\n",
      "           2       0.28      0.23      0.25        43\n",
      "           3       0.17      0.21      0.19        43\n",
      "           4       0.16      0.16      0.16        43\n",
      "           5       0.10      0.09      0.09        43\n",
      "           6       0.20      0.28      0.24        43\n",
      "           7       0.20      0.28      0.23        43\n",
      "           8       0.19      0.19      0.19        43\n",
      "           9       0.08      0.07      0.07        43\n",
      "          10       0.17      0.16      0.17        43\n",
      "          11       0.11      0.07      0.09        43\n",
      "          12       0.03      0.02      0.03        41\n",
      "          13       0.14      0.17      0.15        42\n",
      "          14       0.12      0.12      0.12        42\n",
      "          15       0.06      0.07      0.06        42\n",
      "          16       0.11      0.12      0.12        42\n",
      "          17       0.06      0.07      0.07        42\n",
      "          18       0.14      0.14      0.14        42\n",
      "          19       0.06      0.05      0.05        42\n",
      "          20       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.16       849\n",
      "   macro avg       0.14      0.15      0.15       849\n",
      "weighted avg       0.15      0.16      0.15       849\n",
      "\n",
      "\n",
      "Entrenando modelo Random Forest para agrupación 2...\n",
      "Entrenando modelo para agrupación 2. Clases tras el filtrado: 10\n",
      "Distribución tras SMOTE:\n",
      "target\n",
      "4     344\n",
      "9     344\n",
      "7     344\n",
      "5     344\n",
      "1     344\n",
      "8     344\n",
      "6     344\n",
      "10    344\n",
      "3     344\n",
      "2     344\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultados para agrupación 2:\n",
      "Accuracy: 0.2850\n",
      "F1-Score: 0.2785\n",
      "Recall: 0.2850\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.76      0.69        86\n",
      "           2       0.40      0.45      0.42        85\n",
      "           3       0.31      0.30      0.31        86\n",
      "           4       0.26      0.27      0.26        86\n",
      "           5       0.14      0.16      0.15        86\n",
      "           6       0.12      0.09      0.10        85\n",
      "           7       0.22      0.20      0.21        84\n",
      "           8       0.23      0.26      0.24        84\n",
      "           9       0.26      0.20      0.23        84\n",
      "          10       0.18      0.14      0.16        83\n",
      "\n",
      "    accuracy                           0.29       849\n",
      "   macro avg       0.27      0.28      0.28       849\n",
      "weighted avg       0.28      0.29      0.28       849\n",
      "\n",
      "\n",
      "Entrenando modelo Random Forest para agrupación 4...\n",
      "Entrenando modelo para agrupación 4. Clases tras el filtrado: 5\n",
      "Distribución tras SMOTE:\n",
      "target\n",
      "3    686\n",
      "4    686\n",
      "1    686\n",
      "5    686\n",
      "2    686\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resultados para agrupación 4:\n",
      "Accuracy: 0.4511\n",
      "F1-Score: 0.4437\n",
      "Recall: 0.4511\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.80      0.73       171\n",
      "           2       0.41      0.42      0.41       171\n",
      "           3       0.36      0.36      0.36       170\n",
      "           4       0.40      0.38      0.39       169\n",
      "           5       0.36      0.29      0.32       168\n",
      "\n",
      "    accuracy                           0.45       849\n",
      "   macro avg       0.44      0.45      0.44       849\n",
      "weighted avg       0.44      0.45      0.44       849\n",
      "\n",
      "\n",
      "¡Modelos Random Forest entrenados y guardados exitosamente!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "def entrenar_random_forest(df, agrupacio):\n",
    "  \n",
    "    df = df[df['year'] >= 2014]\n",
    "    \n",
    "    \n",
    "    max_classes = {1: 20, 2: 10, 4: 5}[agrupacio]\n",
    "    df = df[df['target'] <= max_classes]\n",
    "    \n",
    "    print(f\"Entrenando modelo para agrupación {agrupacio}. Clases tras el filtrado: {df['target'].nunique()}\")\n",
    "    \n",
    "   \n",
    "    X = df.drop(columns=['target', 'raceid', 'driverid', 'driverref', 'circuitref', 'positionorder', 'date', 'dob', 'year'])\n",
    "    y = df['target']\n",
    "    \n",
    "   \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"Distribución tras SMOTE:\\n{pd.Series(y_train_smote).value_counts()}\")\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nResultados para agrupación {agrupacio}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "print(\"Entrenando modelo Random Forest para agrupación 1...\")\n",
    "model_rf_agrupacio_1, scaler_rf_1 = entrenar_random_forest(dataframe_single_position, agrupacio=1)\n",
    "\n",
    "print(\"\\nEntrenando modelo Random Forest para agrupación 2...\")\n",
    "model_rf_agrupacio_2, scaler_rf_2 = entrenar_random_forest(dataframe_grouped_by_2, agrupacio=2)\n",
    "\n",
    "print(\"\\nEntrenando modelo Random Forest para agrupación 4...\")\n",
    "model_rf_agrupacio_4, scaler_rf_4 = entrenar_random_forest(dataframe_grouped_by_4, agrupacio=4)\n",
    "\n",
    "joblib.dump(model_rf_agrupacio_1, 'random_forest_model_agrupacio_1.pkl')\n",
    "joblib.dump(scaler_rf_1, 'scaler_rf_agrupacio_1.pkl')\n",
    "\n",
    "joblib.dump(model_rf_agrupacio_2, 'random_forest_model_agrupacio_2.pkl')\n",
    "joblib.dump(scaler_rf_2, 'scaler_rf_agrupacio_2.pkl')\n",
    "\n",
    "joblib.dump(model_rf_agrupacio_4, 'random_forest_model_agrupacio_4.pkl')\n",
    "joblib.dump(scaler_rf_4, 'scaler_rf_agrupacio_4.pkl')\n",
    "\n",
    "print(\"\\n¡Modelos Random Forest entrenados y guardados exitosamente!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
